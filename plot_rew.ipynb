{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but SGDRegressor is expecting 230 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculation of the policy (optimal or greedy action)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(discr)\n\u001b[0;32m---> 41\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     42\u001b[0m     [a\u001b[38;5;241m.\u001b[39mget_action(np\u001b[38;5;241m.\u001b[39marray([x, v, th, av])) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m steps] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m steps\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Plot is initialized\u001b[39;00m\n\u001b[1;32m     46\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "Cell \u001b[0;32mIn [15], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculation of the policy (optimal or greedy action)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(discr)\n\u001b[1;32m     41\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m---> 42\u001b[0m     [a\u001b[38;5;241m.\u001b[39mget_action(np\u001b[38;5;241m.\u001b[39marray([x, v, th, av])) \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m steps] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m steps\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Plot is initialized\u001b[39;00m\n\u001b[1;32m     46\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "Cell \u001b[0;32mIn [15], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculation of the policy (optimal or greedy action)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(discr)\n\u001b[1;32m     41\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m---> 42\u001b[0m     [\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mav\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m steps] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m steps\n\u001b[1;32m     43\u001b[0m ])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Plot is initialized\u001b[39;00m\n\u001b[1;32m     46\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n",
      "File \u001b[0;32m/m/home/home5/58/xuy11/unix/rl_course/ex4/rbf_agent.py:75\u001b[0m, in \u001b[0;36mRBFAgent.get_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     featurized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeaturize(state)\n\u001b[0;32m---> 75\u001b[0m     qs \u001b[39m=\u001b[39m [q\u001b[39m.\u001b[39mpredict(featurized)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_functions]\n\u001b[1;32m     76\u001b[0m     qs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(qs)\n\u001b[1;32m     77\u001b[0m     a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/m/home/home5/58/xuy11/unix/rl_course/ex4/rbf_agent.py:75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     featurized \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeaturize(state)\n\u001b[0;32m---> 75\u001b[0m     qs \u001b[39m=\u001b[39m [q\u001b[39m.\u001b[39;49mpredict(featurized)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_functions]\n\u001b[1;32m     76\u001b[0m     qs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(qs)\n\u001b[1;32m     77\u001b[0m     a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(qs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/rl_course/venv/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:1605\u001b[0m, in \u001b[0;36mBaseSGDRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m   1593\u001b[0m     \u001b[39m\"\"\"Predict using the linear model.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \n\u001b[1;32m   1595\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[39m       Predicted target values per element in X.\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1605\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[0;32m~/rl_course/venv/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:1587\u001b[0m, in \u001b[0;36mBaseSGDRegressor._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[39m\"\"\"Predict using the linear model\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \n\u001b[1;32m   1576\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[39m   Predicted target values per element in X.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1585\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m-> 1587\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m   1589\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m   1590\u001b[0m \u001b[39mreturn\u001b[39;00m scores\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/rl_course/venv/lib/python3.8/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/rl_course/venv/lib/python3.8/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 8 features, but SGDRegressor is expecting 230 features as input."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from rbf_agent import RBFAgent as RBFAgent  \n",
    "import torch\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "env_name = \"CartPole-v0\"\n",
    "resolution = 101  # Resolution of the policy/reward image\n",
    "# env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "agent = RBFAgent.load\n",
    "\n",
    "discr = 25\n",
    "x_min, x_max = env.observation_space.low[0], env.observation_space.high[0]\n",
    "th_min, th_max = env.observation_space.low[2], env.observation_space.high[2]\n",
    "\n",
    "# Fixed values\n",
    "v = 0\n",
    "av = 0\n",
    "a=RBFAgent(2)\n",
    "fp=Path().cwd()\n",
    "a.load(fp)\n",
    "\n",
    "# Discretization of the values\n",
    "x_grid = np.linspace(x_min, x_max, discr)\n",
    "th_grid = np.linspace(th_min, th_max, discr)\n",
    "actions = np.zeros((resolution , resolution ), dtype=np.int32)\n",
    "# Calculation of the policy (optimal or greedy action)\n",
    "steps = range(discr)\n",
    "policy = np.array([\n",
    "    [a.get_action(np.array([x, v, th, av])) for th in steps] for x in steps\n",
    "])\n",
    "\n",
    "# Plot is initialized\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(policy)\n",
    "\n",
    "x_labels = [f\"{np.round(th_grid[th], 2)}\" for th in range(len(th_grid))]\n",
    "y_labels = [f\"{np.round(x_grid[x], 2)}\" for x in range(len(x_grid))]\n",
    "\n",
    "# Ticks on both edges are set\n",
    "ax.set_xticks(np.arange(len(x_labels)))\n",
    "ax.set_yticks(np.arange(len(y_labels)))\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_yticklabels(y_labels)\n",
    "ax.set_xlabel(\"angle (θ)\")\n",
    "ax.set_ylabel(\"position (x)\")\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(x_labels)):\n",
    "    for j in range(len(y_labels)):\n",
    "        action = policy[i, j]\n",
    "        text = ax.text(j, i, action,\n",
    "                       ha=\"center\", va=\"center\", color=\"w\" if action == 0 else \"k\")\n",
    "\n",
    "ax.set_title(f\"Optimal action (Policy) for ẋ={v} and θ˙={av}\")\n",
    "fig.tight_layout()\n",
    "# plt.savefig(\"plots/task-3.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd83ff56f6777e6e758811c394427f18acade90acb82137bfcd1e92d18c6a724"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
